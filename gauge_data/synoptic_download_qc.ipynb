{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e49143",
   "metadata": {},
   "source": [
    "# downloading and quality controlling gauge data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397dec1c",
   "metadata": {},
   "source": [
    "In this notebook, I have an example of the process I used to download and quality control gauge data from the Synoptic data API (https://synopticdata.com/weatherapi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c4771",
   "metadata": {},
   "source": [
    "To run the code, you'll need a Synoptic account. There is a free version available, which is what I use, although it's good to note that there are limitations on the amount and time period of data you can download with the free accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562be9ce",
   "metadata": {},
   "source": [
    "Last thing before we get going is to download this spreadsheet, which I use to translate from Synoptic network IDs to actual network names: https://docs.google.com/spreadsheets/d/1_6kWgqkqEPXW50-1HlkU6ylgGasvyzDO/edit?usp=sharing&ouid=112095056147865565966&rtpof=true&sd=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478aa370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None) # optional, for showing all rows in a table\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature \n",
    "\n",
    "import urllib.request as req\n",
    "import json\n",
    "import requests\n",
    "from tqdm import trange\n",
    "\n",
    "import resample_qc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d6574",
   "metadata": {},
   "source": [
    "### setup for running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb302ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some important path information\n",
    "\n",
    "netID_path = '/path/to/synoptic_netids.xlsx'           # USER INPUT! path to network spreadsheet\n",
    "netids = pd.read_excel(netID_path)\n",
    "\n",
    "using_atlas14 = False                                  # USER INPUT! whether or not to use atlas14 data for QC\n",
    "atlas14_path = '/path/to/atlas14_data/'                # USER INPUT! path to atlas14 (if using, otherwise ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for downloading data from Synoptic\n",
    "\n",
    "start = '202308180000'      # USER INPUT! start date and time in YYYYMMDDHHMM\n",
    "end   = '202308230000'      # USER INPUT! end date and time in YYYYMMDDHHMM\n",
    "api_token = ''              # USER INPUT! API token associated with synoptic data account\n",
    "lat_bounds = [32, 38]       # USER INPUT! lower and upper latitude boundaries for study area\n",
    "lon_bounds = [-123, -114]   # USER INPUT! lower and upper longitude boundaries for study area\n",
    "units = 'metric'            # USER INPUT! units - 'metric' for mm and 'english' for inches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2a0be",
   "metadata": {},
   "source": [
    "### Download gauge data from Synoptic API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download gauge data for desired time period \n",
    "\n",
    "lat_list = np.arange(lat_bounds[0], lat_bounds[1]+0.1, 0.2) # break study area into latitude bands\n",
    "dict_list = [] # define a list for data to be stored in\n",
    "\n",
    "for i in trange(len(lat_list) - 1): # loop over lat bands\n",
    "    # define API request URL\n",
    "    bbox = f'{lon_bounds[0]},{lat_list[i]},{lon_bounds[1]},{lat_list[i+1]}' # area for data download\n",
    "    api_root = 'https://api.synopticdata.com/v2/stations/timeseries'\n",
    "    api_params = ( \n",
    "        f'?token={api_token}&'\n",
    "        f'start={start}&end={end}&'\n",
    "        f'bbox={bbox}&'\n",
    "        f'vars=precip&precip=1&all_reports=1&'\n",
    "        f'units={units}'\n",
    "    )\n",
    "    api_request_url = api_root + api_params\n",
    "    \n",
    "    # request and download data\n",
    "    response = req.urlopen(api_request_url)\n",
    "    api_text_data = response.read() \n",
    "    data_dict = json.loads(api_text_data)\n",
    "    \n",
    "    # check for errors\n",
    "    if list(data_dict.keys()) == ['SUMMARY']:\n",
    "        print(data_dict['SUMMARY']['RESPONSE_MESSAGE'])\n",
    "        continue\n",
    "    \n",
    "    # add data from lat band to list\n",
    "    station_list_partial = data_dict['STATION']\n",
    "    dict_list.append(station_list_partial)\n",
    "    \n",
    "# combine data from all lat bands into one list\n",
    "station_list = sum(dict_list, [])\n",
    "print('number of stations: ', len(station_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa856823",
   "metadata": {},
   "source": [
    "### check that all stations downloaded have precipitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for stations with no interval precip data\n",
    "for station in station_list:\n",
    "    if 'precip_intervals_set_1d' not in station['OBSERVATIONS'].keys():\n",
    "        print('no intervals: ', station['STID'])\n",
    "        \n",
    "# check for stations with no accumulated precip data\n",
    "for station in station_list:\n",
    "    if 'precip_accumulated_set_1d' not in station['OBSERVATIONS'].keys():\n",
    "        print('no accumulated: ', station['STID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127513b8",
   "metadata": {},
   "source": [
    "### resample and perform quality control on station data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a60c4",
   "metadata": {},
   "source": [
    "NOTE: for help with the resample_qc.py functions, you can look at the resample_qc.py file, or take a look at the description of the functiom (for example, by running the line\n",
    "? resample_qc.resample \n",
    "to look at the doc string of the resample function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8349aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataframe to store gauge information/metrics\n",
    "df_all = pd.DataFrame(columns=['STID','Name','Network','Latitude','Longitude',\n",
    "                               'Storm_Total','Max_Intensity','QC'])\n",
    "# set up dictionary to store gauge data\n",
    "resampled_data_dict = {}\n",
    "\n",
    "# loop through each station\n",
    "for i in trange(len(station_list)):\n",
    "    # get station info\n",
    "    station = station_list[i]                                       # select station\n",
    "    stid = station['STID']                                          # station ID\n",
    "    name = station['NAME']                                          # station name\n",
    "    lat = float(station['LATITUDE'])                                # latutude\n",
    "    lon = float(station['LONGITUDE'])                               # longitude\n",
    "    mnet_id = int(station['MNET_ID'])                               # mesonet ID\n",
    "    network = netids['Name'].loc[netids['ID'] == mnet_id].values[0] # network, from mesonet ID\n",
    "\n",
    "    # resample to hourly\n",
    "    df_raw = pd.DataFrame.from_dict(station['OBSERVATIONS'])\n",
    "    datetimes_raw = df_raw['date_time']\n",
    "    precip_raw = df_raw['precip_intervals_set_1d']\n",
    "    df_resampled = resample_qc.resample(precip_raw, datetimes_raw)\n",
    "\n",
    "    # find maximum hourlt intensity and total storm accumulation\n",
    "    maximum = df_resampled.max()\n",
    "    total = precip_raw.sum()\n",
    "\n",
    "    # quality control\n",
    "    qc_flag = resample_qc.quality_control(\n",
    "        precip_raw, datetimes_raw, \n",
    "        PFDS=using_atlas14,\n",
    "        PFDS_folder=atlas14_path, \n",
    "        lat=lat, lon=lon)\n",
    "    \n",
    "    # store station info to dataframe\n",
    "    df_all.loc[i] = [stid, name, network, lat, lon, total, maximum, qc_flag]\n",
    "    # store resampled hourly data to dictionary \n",
    "    resampled_data_dict[stid] = df_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f8a9bd",
   "metadata": {},
   "source": [
    "### always good to check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387351b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a look at the stations that passed quality control\n",
    "df_all[df_all['QC']==0].sort_values(by='Storm_Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the stations that did NOT pass quality control\n",
    "df_all[df_all['QC']!=0].sort_values(by='Storm_Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0979a",
   "metadata": {},
   "source": [
    "### save the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6f844",
   "metadata": {},
   "source": [
    "You can save the gauge information dataframe, but note that this does not contain the hourly gauge data, just the accumulations and max intensities. Below is also code for creating and saving a netCDF file, which stores gauge information (such as station ID, location, network, and QC flag), along with the resampled hourly data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save information dataframe, if desired\n",
    "df_all.to_csv('gauge_info_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data to save to netCDF\n",
    "hourly_data = []\n",
    "\n",
    "start_dt = datetime.strptime(start, '%Y%m%d%H%M')\n",
    "end_dt = datetime.strptime(end, '%Y%m%d%H%M')\n",
    "time_array = np.arange(start_dt, end_dt, dtype='datetime64[h]')\n",
    "\n",
    "for i in df_all.index:\n",
    "    stid = df_all['STID'].loc[i]\n",
    "\n",
    "    precip_resample = resampled_data_dict[stid].copy()\n",
    "    precip_resample.index = precip_resample.index.values.astype('datetime64[h]')\n",
    "    precip_resample_reindex = precip_resample.reindex(time_array)\n",
    "    precip_resample_vals = precip_resample_reindex.values\n",
    "    hourly_data.append(precip_resample_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put station data and information into an xarray dataset\n",
    "ds = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "        precip_int=(['station', 'time'], hourly_data),\n",
    "        total=(['station'], df_all['Storm_Total'].values),\n",
    "        maximum=(['station'], df_all['Max_Intensity'].values),\n",
    "        network=(['station'], df_all['Network'].values ),\n",
    "        qc_flag=(['station'], df_all['QC'].values )\n",
    "    ),\n",
    "    coords=dict(\n",
    "        station = df_all['STID'].values,\n",
    "        time=time_array,\n",
    "        lon=('station', df_all['Longitude'].values),\n",
    "        lat=('station', df_all['Latitude'].values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset to netCDF file\n",
    "ds.to_netcdf('mesowest_gaugedata_hourly.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env2)",
   "language": "python",
   "name": "env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
